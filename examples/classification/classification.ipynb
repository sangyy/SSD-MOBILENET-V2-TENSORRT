{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLT Classification example usecase\n",
    "\n",
    "This notebook shows an example use case for classification using the Transfer Learning Toolkit.\n",
    "\n",
    "0. [Set up env variables](#head-0)\n",
    "1. [Prepare dataset and pretrained model](#head-1)\n",
    "    1. [Split the dataset into train/test/val](#head-1-1)\n",
    "    2. [Download pre-trained model](#head-1-2)\n",
    "2. [Provide training specfication](#head-2)\n",
    "3. [Run TLT training](#head-3)\n",
    "4. [Evaluate trained models](#head-4)\n",
    "5. [Prune trained models](#head-5)\n",
    "6. [Retrain pruned models](#head-6)\n",
    "7. [Testing the model](#head-7)\n",
    "8. [Visualize inferences](#head-8)\n",
    "9. [Export and Deploy!](#head-9)\n",
    "    1. [Int8 Optimization](#head-9-1)\n",
    "    2. [Generate TensorRT engine](#head-9-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup env variables <a class=\"anchor\" id=\"head-0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please enter your key in place of $KEY.\n",
    "%env USER_EXPERIMENT_DIR=/workspace/tlt-experiments/classification\n",
    "%env DATA_DOWNLOAD_DIR=/workspace/tlt-experiments/data\n",
    "%env SPECS_DIR=/workspace/examples/classification/specs\n",
    "%env KEY=YOUR_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare datasets and pre-trained model <a class=\"anchor\" id=\"head-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the pascal VOC dataset for the tutorial. To find more details please visit \n",
    "http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit. Please download the dataset present at http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to $DATA_DOWNLOAD_DIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that file is present\n",
    "import os\n",
    "DATA_DIR = os.environ.get('DATA_DOWNLOAD_DIR')\n",
    "if not os.path.isfile(os.path.join(DATA_DIR , 'VOCtrainval_11-May-2012.tar')):\n",
    "    print('tar file for dataset not found. Please download.')\n",
    "else:\n",
    "    print('Found dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack \n",
    "!tar -xvf $DATA_DOWNLOAD_DIR/VOCtrainval_11-May-2012.tar -C $DATA_DOWNLOAD_DIR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "!ls $DATA_DOWNLOAD_DIR/VOCdevkit/VOC2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Split the dataset into train/val/test <a class=\"anchor\" id=\"head-1-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pascal VOC Dataset is converted to our format (for classification) and then to train/val/test in the next two blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as join_path\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "DATA_DIR=os.environ.get('DATA_DOWNLOAD_DIR')\n",
    "source_dir = join_path(DATA_DIR, \"VOCdevkit/VOC2012\")\n",
    "target_dir = join_path(DATA_DIR, \"formatted\")\n",
    "\n",
    "\n",
    "suffix = '_trainval.txt'\n",
    "classes_dir = join_path(source_dir, \"ImageSets\", \"Main\")\n",
    "images_dir = join_path(source_dir, \"JPEGImages\")\n",
    "classes_files = glob.glob(classes_dir+\"/*\"+suffix)\n",
    "for file in classes_files:\n",
    "    # get the filename and make output class folder\n",
    "    classname = os.path.basename(file)\n",
    "    if classname.endswith(suffix):\n",
    "        classname = classname[:-len(suffix)]\n",
    "        target_dir_path = join_path(target_dir, classname)\n",
    "        if not os.path.exists(target_dir_path):\n",
    "            os.makedirs(target_dir_path)\n",
    "    else:\n",
    "        continue\n",
    "    print(classname)\n",
    "\n",
    "\n",
    "    with open(file) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "\n",
    "    for line in content:\n",
    "        tokens = re.split('\\s+', line)\n",
    "        if tokens[1] == '1':\n",
    "            # copy this image into target dir_path\n",
    "            target_file_path = join_path(target_dir_path, tokens[0] + '.jpg')\n",
    "            src_file_path = join_path(images_dir, tokens[0] + '.jpg')\n",
    "            shutil.copyfile(src_file_path, target_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from random import shuffle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "DATA_DIR=os.environ.get('DATA_DOWNLOAD_DIR')\n",
    "SOURCE_DIR=join_path(DATA_DIR, 'formatted')\n",
    "TARGET_DIR=os.path.join(DATA_DIR,'split')\n",
    "# list dir\n",
    "dir_list = os.walk(SOURCE_DIR).next()[1]\n",
    "# for each dir, create a new dir in split\n",
    "for dir_i in tqdm(dir_list):\n",
    "        newdir_train = os.path.join(TARGET_DIR, 'train', dir_i)\n",
    "        newdir_val = os.path.join(TARGET_DIR, 'val', dir_i)\n",
    "        newdir_test = os.path.join(TARGET_DIR, 'test', dir_i)\n",
    "        \n",
    "        if not os.path.exists(newdir_train):\n",
    "                os.makedirs(newdir_train)\n",
    "        if not os.path.exists(newdir_val):\n",
    "                os.makedirs(newdir_val)\n",
    "        if not os.path.exists(newdir_test):\n",
    "                os.makedirs(newdir_test)\n",
    "\n",
    "        img_list = glob.glob(os.path.join(SOURCE_DIR, dir_i, '*.jpg'))\n",
    "        # shuffle data\n",
    "        shuffle(img_list)\n",
    "\n",
    "        for j in range(int(len(img_list)*0.7)):\n",
    "                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'train', dir_i))\n",
    "\n",
    "        for j in range(int(len(img_list)*0.7), int(len(img_list)*0.8)):\n",
    "                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'val', dir_i))\n",
    "                \n",
    "        for j in range(int(len(img_list)*0.8), len(img_list)):\n",
    "                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'test', dir_i))\n",
    "                \n",
    "print('Done splitting dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $DATA_DOWNLOAD_DIR/split/test/cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Download pretrained models <a class=\"anchor\" id=\"head-1-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ngc registry model list nvidia/tlt_pretrained_classification:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $USER_EXPERIMENT_DIR/pretrained_resnet18/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tlt_pretrained_classification:resnet18 --dest $USER_EXPERIMENT_DIR/pretrained_resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $USER_EXPERIMENT_DIR/pretrained_resnet18/tlt_pretrained_classification_vresnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Provide training specfication <a class=\"anchor\" id=\"head-2\"></a>\n",
    "* Training dataset\n",
    "* Validation dataset\n",
    "* Pre-trained models\n",
    "* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat $SPECS_DIR/classification_spec.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run TLT training <a class=\"anchor\" id=\"head-3\"></a>\n",
    "* Provide the sample spec file and the output directory location for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-train classification -e $SPECS_DIR/classification_spec.cfg -r $USER_EXPERIMENT_DIR/output -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To run this training using multiple GPU's, please uncomment the line below and \"\n",
    "      \"update the --gpus parameter to the number of GPU's you wish to use.\")\n",
    "# !tlt-train classification -e $SPECS_DIR/classification_spec.cfg \\\n",
    "#                           -r $USER_EXPERIMENT_DIR/output \\\n",
    "#                           -k $KEY --gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"To resume from a checkpoint, use --init_epoch along with your checkpoint configured in the spec file.\")\n",
    "print(\"Please make sure that the model_path in the spec file is now updated to the '.tlt' file of the corresponding\"\n",
    "      \"epoch you wish to resume from. You may choose from the files found under, '$USER_EXPERIMENT_DIR/output/weights' folder.\")\n",
    "# !tlt-train classification -e $SPECS_DIR/classification_spec.cfg \\\n",
    "#                           -r $USER_EXPERIMENT_DIR/output \\\n",
    "#                           -k $KEY --gpus 2 \\\n",
    "#                           --init_epoch N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate trained models <a class=\"anchor\" id=\"head-4\"></a>\n",
    "\n",
    "In this step, we assume that the training is complete and the model from the final epoch (`resnet_080.tlt`) is available. If you would like to run evaluation on an earlier model, please edit the spec file at `$SPECS_DIR/classification_spec.cfg` to point to the intended model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-evaluate classification -e $SPECS_DIR/classification_spec.cfg -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prune trained models <a class=\"anchor\" id=\"head-5\"></a>\n",
    "* Specify pre-trained model\n",
    "* Equalization criterion\n",
    "* Threshold for pruning\n",
    "* Exclude prediction layer that you don't want pruned (e.g. predictions)\n",
    "\n",
    "Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold to use is depend on the dataset. A pth value 0.68 is just a starting point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the checkpoint epoch number of the model to be used for the pruning.\n",
    "# This should be lesser than the number of epochs training has been run for, incase training was interrupted earlier.\n",
    "# By default, the default final model is at epoch 080.\n",
    "%env EPOCH=080\n",
    "!mkdir -p $USER_EXPERIMENT_DIR/output/resnet_pruned\n",
    "!tlt-prune -m $USER_EXPERIMENT_DIR/output/weights/resnet_$EPOCH.tlt \\\n",
    "           -o $USER_EXPERIMENT_DIR/output/resnet_pruned/resnet18_nopool_bn_pruned.tlt \\\n",
    "           -eq union \\\n",
    "           -pth 0.68 \\\n",
    "           -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pruned model:')\n",
    "print('------------')\n",
    "!ls -r1t $USER_EXPERIMENT_DIR/output/resnet_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrain pruned models <a class=\"anchor\" id=\"head-6\"></a>\n",
    "* Model needs to be re-trained to bring back accuracy after pruning\n",
    "* Specify re-training specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $SPECS_DIR/classification_retrain_spec.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-train classification -e $SPECS_DIR/classification_retrain_spec.cfg \\\n",
    "                          -r $USER_EXPERIMENT_DIR/output_retrain \\\n",
    "                          -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing the model! <a class=\"anchor\" id=\"head-7\"></a>\n",
    "\n",
    "In this step, we assume that the training is complete and the model from the final epoch (`resnet_080.tlt`) is available. If you would like to run evaluation on an earlier model, please edit the spec file at `$SPECS_DIR/classification_retrain_spec.cfg` to point to the intended model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-evaluate classification -e $SPECS_DIR/classification_retrain_spec.cfg -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Inferences <a class=\"anchor\" id=\"head-8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the output results of our model on test images, we can use the `tlt-infer` tool. Note that using models trained for higher epochs will usually result in better results. First we'll run inference in single image mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing a random test image from the test set.\n",
    "import os\n",
    "import random\n",
    "\n",
    "test_dataset = os.path.join(os.environ.get('DATA_DOWNLOAD_DIR'), 'split', 'test')\n",
    "classes = [item for item in os.listdir(test_dataset) if os.path.isdir(os.path.join(test_dataset,item))]\n",
    "class_under_test = random.choice(classes)\n",
    "test_image_dir = os.path.join(test_dataset, class_under_test)\n",
    "image_list = [os.path.join(test_image_dir, item) for item in os.listdir(test_image_dir)\n",
    "              if item.endswith('.jpg')]\n",
    "os.environ['TEST_IMAGE'] = random.choice(image_list)\n",
    "\n",
    "print(\"Input image is from class: {}\".format(class_under_test))\n",
    "print(\"Image path is: {}\".format(os.environ['TEST_IMAGE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the checkpoint epoch number to use for the subsequent steps.\n",
    "# This should be lesser than the number of epochs training has been run for, incase training was interrupted earlier.\n",
    "# By default, the default final model is at epoch 080.\n",
    "%env EPOCH=080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-infer classification -m $USER_EXPERIMENT_DIR/output_retrain/weights/resnet_$EPOCH.tlt \\\n",
    "                          -k $KEY -b 32 -i $TEST_IMAGE \\\n",
    "                          -cm $USER_EXPERIMENT_DIR/output_retrain/classmap.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run inference in directory mode to run on a set of test images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-infer classification -m $USER_EXPERIMENT_DIR/output_retrain/weights/resnet_$EPOCH.tlt \\\n",
    "                          -k $KEY -b 32 -d $DATA_DOWNLOAD_DIR/split/test/person \\\n",
    "                          -cm $USER_EXPERIMENT_DIR/output_retrain/classmap.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in Getting Started Guide, this outputs a results.csv file in the same directory. We can use a simple python program to see the visualize the output of csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "import os\n",
    "import csv\n",
    "from math import ceil\n",
    "\n",
    "DATA_DIR = os.environ.get('DATA_DOWNLOAD_DIR')\n",
    "csv_path = os.path.join(DATA_DIR, 'split', 'test', 'person', 'result.csv')\n",
    "results = []\n",
    "with open(csv_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        results.append((row[0], row[1]))\n",
    "\n",
    "w,h = 200,200\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "columns = 5\n",
    "rows = 1\n",
    "for i in range(1, columns*rows + 1):\n",
    "    ax = fig.add_subplot(rows, columns,i)\n",
    "    img = Image.open(results[i][0])\n",
    "    img = img.resize((w,h), Image.ANTIALIAS)\n",
    "    plt.imshow(img)\n",
    "    ax.set_title(results[i][1], fontsize=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export and Deploy! <a class=\"anchor\" id=\"head-9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-export classification \\\n",
    "            -m $USER_EXPERIMENT_DIR/output_retrain/weights/resnet_$EPOCH.tlt \\\n",
    "            -o $USER_EXPERIMENT_DIR/export/final_model.etlt \\\n",
    "            -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exported model:')\n",
    "print('------------')\n",
    "!ls -lh $USER_EXPERIMENT_DIR/export/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Int8 Optimization <a class=\"anchor\" id=\"head-9-1\"></a>\n",
    "Classification model supports int8 optimization for inference in TRT. Inorder to use this, we must calibrate the model to run 8-bit inferences. This involves 2 steps\n",
    "\n",
    "* Generate calibration tensorfile from the training data using tlt-int8-tensorfile\n",
    "* Use tlt-export to generate int8 calibration table.\n",
    "\n",
    "*Note: For this example, we generate a calibration tensorfile containing 10 batches of training data.\n",
    "Ideally, it is best to use atleast 10-20% of the training data to calibrate the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-int8-tensorfile classification -e $SPECS_DIR/classification_retrain_spec.cfg \\\n",
    "                                    -m 10 \\\n",
    "                                    -o $USER_EXPERIMENT_DIR/export/calibration.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the pre-existing exported .etlt file.\n",
    "!rm -rf $USER_EXPERIMENT_DIR/export/final_model.etlt\n",
    "!tlt-export classification \\\n",
    "            -m $USER_EXPERIMENT_DIR/output_retrain/weights/resnet_$EPOCH.tlt \\\n",
    "            -o $USER_EXPERIMENT_DIR/export/final_model.etlt \\\n",
    "            -k $KEY \\\n",
    "            --cal_data_file $USER_EXPERIMENT_DIR/export/calibration.tensor \\\n",
    "            --data_type int8 \\\n",
    "            --batches 10 \\\n",
    "            --cal_cache_file $USER_EXPERIMENT_DIR/export/final_model_int8_cache.bin \\\n",
    "            -v "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Generate TensorRT engine <a class=\"anchor\" id=\"head-9-2\"></a>\n",
    "Verify engine generation using the `tlt-converter` utility included with the docker.\n",
    "\n",
    "The `tlt-converter` produces optimized tensorrt engines for the platform that it resides on. Therefore, to get maximum performance, please instantiate this docker and execute the `tlt-converter` command, with the exported `.etlt` file and calibration cache (for int8 mode) on your target device. The converter utility included in this docker only works for x86 devices, with discrete NVIDIA GPU's. \n",
    "\n",
    "For the jetson devices, please download the converter for jetson from the dev zone link [here](https://developer.nvidia.com/tlt-converter). \n",
    "\n",
    "If you choose to integrate your model into deepstream directly, you may do so by simply copying the exported `.etlt` file along with the calibration cache to the target device and updating the spec file that configures the `gst-nvinfer` element to point to this newly exported model. Usually this file is called `config_infer_primary.txt` for detection models and `config_infer_secondary_*.txt` for classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-converter $USER_EXPERIMENT_DIR/export/final_model.etlt \\\n",
    "               -k $KEY \\\n",
    "               -c $USER_EXPERIMENT_DIR/export/final_model_int8_cache.bin \\\n",
    "               -o predictions/Softmax \\\n",
    "               -d 3,224,224 \\\n",
    "               -i nchw \\\n",
    "               -m 64 -t int8 \\\n",
    "               -e $USER_EXPERIMENT_DIR/export/final_model.trt \\\n",
    "               -b 64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}