{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # TLT FasterRCNN example usecase\n",
    "\n",
    " This notebook shows an example usecase of FasterRCNN using Transfer Learning Toolkit.\n",
    "\n",
    " 0. [Set up env variables](#head-0)\n",
    " 1. [Prepare dataset and pretrained model](#head-1)<br>\n",
    "     1.1 [Download pretrained model](#head-1-1)<br>\n",
    " 2. [Provide training specification](#head-2)\n",
    " 3. [Run TLT training](#head-3)\n",
    " 4. [Evaluate trained models](#head-4)\n",
    " 5. [Prune trained models](#head-5)\n",
    " 6. [Retrain pruned models](#head-6)\n",
    " 7. [Evaluate retrained model](#head-7)\n",
    " 8. [Visualize inferences](#head-8)\n",
    " 9. [Deploy](#head-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 0. Set up env variables <a class=\"anchor\" id=\"head-0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up env variables for cleaner command line commands.\n",
    "print(\"Please replace the variables with your own.\")\n",
    "%env KEY=tlt\n",
    "%env USER_EXPERIMENT_DIR=/workspace/tlt-experiments\n",
    "%env DATA_DOWNLOAD_DIR=/workspace/tlt-experiments/data\n",
    "%env SPECS_DIR=./specs\n",
    "!mkdir -p $DATA_DOWNLOAD_DIR/faster_rcnn\n",
    "# Prepend current directory and HOME directory to the PATH env variable.\n",
    "import os\n",
    "os.environ['PATH'] = './:' + os.environ.get('HOME', '') + ':' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Prepare dataset and pretrained model <a class=\"anchor\" id=\"head-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will be using the KITTI detection dataset for the tutorial. To find more details please visit\n",
    " http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d. Please download the KITTI detection images (http://www.cvlibs.net/download.php?file=data_object_image_2.zip) and labels (http://www.cvlibs.net/download.php?file=data_object_label_2.zip) to $DATA_DOWNLOAD_DIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset is present\n",
    "!mkdir -p $DATA_DOWNLOAD_DIR\n",
    "!if [ ! -f $DATA_DOWNLOAD_DIR/data_object_image_2.zip ]; then echo 'Image zip file not found, please download.'; else echo 'Found Image zip file.';fi\n",
    "!if [ ! -f $DATA_DOWNLOAD_DIR/data_object_label_2.zip ]; then echo 'Label zip file not found, please download.'; else echo 'Found Labels zip file.';fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack \n",
    "!unzip -u $DATA_DOWNLOAD_DIR/data_object_image_2.zip -d $DATA_DOWNLOAD_DIR\n",
    "!unzip -u $DATA_DOWNLOAD_DIR/data_object_label_2.zip -d $DATA_DOWNLOAD_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "!ls -l $DATA_DOWNLOAD_DIR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, if you have your own dataset already in a volume (or folder), you can mount the volume on `DATA_DOWNLOAD_DIR` (or create a soft link). Below shows an example:\n",
    "```bash\n",
    "# if your dataset is in /dev/sdc1\n",
    "mount /dev/sdc1 $DATA_DOWNLOAD_DIR\n",
    "\n",
    "# if your dataset is in folder /var/dataset\n",
    "ln -sf /var/dataset $DATA_DOWNLOAD_DIR\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Prepare tfrecords from kitti format dataset <a class=\"anchor\" id=\"head-1-1\"></a>\n",
    "\n",
    "* Update the tfrecords spec file to take in your kitti format dataset\n",
    "* Create the tfrecords using the tlt-dataset-convert \n",
    "* TFRecords only need to be generated once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TFrecords conversion spec file for training\")\n",
    "!cat $SPECS_DIR/frcnn_tfrecords_kitti_trainval.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new directory for the output tfrecords dump.\n",
    "!mkdir -p $USER_EXPERIMENT_DIR/tfrecords\n",
    "#KITTI trainval\n",
    "!tlt-dataset-convert -d $SPECS_DIR/frcnn_tfrecords_kitti_trainval.txt \\\n",
    "                     -o $USER_EXPERIMENT_DIR/tfrecords/kitti_trainval/kitti_trainval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $USER_EXPERIMENT_DIR/tfrecords/kitti_trainval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.2 Download pre-trained model <a class=\"anchor\" id=\"head-1-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc registry model list nvidia/tlt_pretrained_object_detection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model from NGC.\n",
    "!ngc registry model download-version nvidia/tlt_pretrained_object_detection:resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Copy weights to data directory.\n",
    "!cp tlt_pretrained_object_detection_vresnet18/resnet_18.hdf5 $DATA_DOWNLOAD_DIR/faster_rcnn/\n",
    "!rm -rf tlt_pretrained_object_detection_vresnet18\n",
    "!ls -rlt $DATA_DOWNLOAD_DIR/faster_rcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Provide training specification <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's/$KEY/'\"$KEY/g\" $SPECS_DIR/default_spec_resnet18.txt\n",
    "!cat $SPECS_DIR/default_spec_resnet18.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Run TLT training <a class=\"anchor\" id=\"head-3\"></a>\n",
    " * Provide the sample spec file for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tlt-train faster_rcnn -e $SPECS_DIR/default_spec_resnet18.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model for each epoch:')\n",
    "print('---------------------')\n",
    "!ls -lh $USER_EXPERIMENT_DIR/data/faster_rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For multi-GPU, please uncomment and run this instead. Change --gpus based on your machine.\")\n",
    "# !tlt-train faster_rcnn -e $SPECS_DIR/default_spec_resnet18.txt \\\n",
    "#                        --gpus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For resume training from checkpoint, please uncomment and run this instead. Change/Add the 'resume_from_model' field in the spec file.\")\n",
    "# !tlt-train faster_rcnn -e $SPECS_DIR/default_spec_resnet18.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Evaluate trained models <a class=\"anchor\" id=\"head-4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-evaluate faster_rcnn -e $SPECS_DIR/default_spec_resnet18.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Prune trained models <a class=\"anchor\" id=\"head-5\"></a>\n",
    " * Specify pre-trained model\n",
    " * Equalization criterion\n",
    " * Threshold for pruning\n",
    " * A key to save and load the model\n",
    " * Output directory to store the model\n",
    " \n",
    "Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold to use is depend on the dataset. A pth value 0.4 is just a start point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-prune -m $USER_EXPERIMENT_DIR/data/faster_rcnn/frcnn_kitti_resnet18.epoch12.tlt \\\n",
    "           -o $USER_EXPERIMENT_DIR/data/faster_rcnn/model_1_pruned.tlt  \\\n",
    "           -eq union  \\\n",
    "           -pth 0.4 \\\n",
    "           -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt $USER_EXPERIMENT_DIR/data/faster_rcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Retrain pruned models <a class=\"anchor\" id=\"head-6\"></a>\n",
    " * Model needs to be re-trained to bring back accuracy after pruning\n",
    " * Specify re-training specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have updated the spec file to include the newly pruned model as a pretrained weights.\n",
    "!sed -i 's/$KEY/'\"$KEY/g\" $SPECS_DIR/default_spec_resnet18_retrain_spec.txt\n",
    "!cat $SPECS_DIR/default_spec_resnet18_retrain_spec.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining using the pruned model as pretrained weights \n",
    "!tlt-train faster_rcnn -e $SPECS_DIR/default_spec_resnet18_retrain_spec.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the newly retrained model.\n",
    "!ls -rlt $USER_EXPERIMENT_DIR/data/faster_rcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Evaluate retrained model <a class=\"anchor\" id=\"head-7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt-evaluate faster_rcnn -e $SPECS_DIR/default_spec_resnet18_retrain_spec.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Visualize inferences <a class=\"anchor\" id=\"head-8\"></a>\n",
    " In this section, we run the tlt-infer tool to generate inferences on the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running inference for detection on n images\n",
    "# Please go to $USER_EXPERIMENT_DIR/data/faster_rcnn/inference_results_imgs_retrain to see the visualizatons.\n",
    "!tlt-infer faster_rcnn -e $SPECS_DIR/default_spec_resnet18_retrain_spec.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tlt-infer` tool produces two outputs. \n",
    "1. Overlain images in `$USER_EXPERIMENT_DIR/data/faster_rcnn/inference_results_imgs_retrain`\n",
    "2. Frame by frame bbox labels in kitti format located in `$USER_EXPERIMENT_DIR/data/faster_rcnn/inference_dump_labels_retrain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid visualizer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from math import ceil\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n",
    "\n",
    "def visualize_images(image_dir, num_cols=4, num_images=10):\n",
    "    output_path = os.path.join(os.environ['USER_EXPERIMENT_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx / num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images.\n",
    "OUTPUT_PATH = 'data/faster_rcnn/inference_results_imgs_retrain' # relative path from $USER_EXPERIMENT_DIR.\n",
    "COLS = 3 # number of columns in the visualizer grid.\n",
    "IMAGES = 9 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9. Deploy! <a class=\"anchor\" id=\"head-9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export in FP32 mode. \\\n",
    "!tlt-export faster_rcnn -m $USER_EXPERIMENT_DIR/data/faster_rcnn/frcnn_kitti_resnet18_retrain.epoch12.tlt  \\\n",
    "                        -o $USER_EXPERIMENT_DIR/data/faster_rcnn/frcnn_kitti_resnet18_retrain.etlt \\\n",
    "                        -e $SPECS_DIR/default_spec_resnet18_retrain_spec.txt \\\n",
    "                        -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export in FP16 mode. \\\n",
    "# Note that the .etlt model in FP16 mode is  \\\n",
    "# the same as in FP32 mode. \\\n",
    "!tlt-export faster_rcnn -m $USER_EXPERIMENT_DIR/data/faster_rcnn/frcnn_kitti_resnet18_retrain.epoch12.tlt  \\\n",
    "                        -o $USER_EXPERIMENT_DIR/data/faster_rcnn/frcnn_kitti_resnet18_retrain_fp16.etlt \\\n",
    "                        -e $SPECS_DIR/default_spec_resnet18_retrain_spec.txt \\\n",
    "                        -k $KEY \\\n",
    "                        --data_type fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export in INT8 mode(generate calibration cache file). \\\n",
    "# Note that the .etlt model in INT8 mode is the same as \\\n",
    "# in FP32 mode. \\\n",
    "!tlt-export faster_rcnn -m $USER_EXPERIMENT_DIR/data/faster_rcnn/frcnn_kitti_resnet18_retrain.epoch12.tlt  \\\n",
    "                        -o $USER_EXPERIMENT_DIR/data/faster_rcnn/frcnn_kitti_resnet18_retrain_int8.etlt \\\n",
    "                        -e $SPECS_DIR/default_spec_resnet18_retrain_spec.txt \\\n",
    "                        -k $KEY \\\n",
    "                        --cal_image_dir  $USER_EXPERIMENT_DIR/data/testing/image_2 \\\n",
    "                        --data_type int8 \\\n",
    "                        --batch_size 8 \\\n",
    "                        --batches 10 \\\n",
    "                        --cal_cache_file $USER_EXPERIMENT_DIR/data/faster_rcnn/cal.bin  \\\n",
    "                        --cal_data_file $USER_EXPERIMENT_DIR/data/faster_rcnn/cal.tensorfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to TensorRT engine(FP32) is omitted here as this is trivial.\n",
    "# Convert to TensorRT engine(FP16).\n",
    "# Specify the GPU ID when generating the TensorRT engine and do inference,\n",
    "# in case there are different GPU types on the machine.\n",
    "# Make sure your GPU type supports the FP16 data type before running this cell.\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "!tlt-converter -k $KEY  \\\n",
    "               -d 3,384,1248 \\\n",
    "               -o dense_class_td/Softmax,dense_regress_td/BiasAdd,proposal \\\n",
    "               -e $USER_EXPERIMENT_DIR/data/faster_rcnn/trt.fp16.engine \\\n",
    "               -m 4 \\\n",
    "               -t fp16 \\\n",
    "               -i nchw \\\n",
    "               $USER_EXPERIMENT_DIR/data/faster_rcnn/frcnn_kitti_resnet18_retrain_fp16.etlt \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorRT engine(INT8).\n",
    "# Specify the GPU ID when generating the TensorRT engine and do inference,\n",
    "# in case there are different GPU types on the machine.\n",
    "# Make sure your GPU type supports the INT8 data type before running this cell.\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "!tlt-converter -k $KEY  \\\n",
    "               -d 3,384,1248 \\\n",
    "               -o dense_class_td/Softmax,dense_regress_td/BiasAdd,proposal \\\n",
    "               -c $USER_EXPERIMENT_DIR/data/faster_rcnn/cal.bin \\\n",
    "               -e $USER_EXPERIMENT_DIR/data/faster_rcnn/trt.int8.engine \\\n",
    "               -b 8 \\\n",
    "               -m 4 \\\n",
    "               -t int8 \\\n",
    "               -i nchw \\\n",
    "               $USER_EXPERIMENT_DIR/data/faster_rcnn/frcnn_kitti_resnet18_retrain_int8.etlt \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Exported model and converted TensorRT engine:')\n",
    "print('------------')\n",
    "!ls -lh $USER_EXPERIMENT_DIR/data/faster_rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do inference with TensorRT on the generated TensorRT engine\n",
    "# Please go to $USER_EXPERIMENT_DIR/data/faster_rcnn/inference_results_imgs_retrain to see the visualizatons.\n",
    "# Here we use the INT8 engine for inference, if you want to use FP16 engine instead please\n",
    "# customize the 'trt_engine' parameter in the spec file below to point to the FP16 engine.\n",
    "!sed -i s/#trt/trt/g $SPECS_DIR/default_spec_resnet18_retrain_spec.txt\n",
    "!sed -i s/#}/}/g $SPECS_DIR/default_spec_resnet18_retrain_spec.txt\n",
    "!tlt-infer faster_rcnn -e $SPECS_DIR/default_spec_resnet18_retrain_spec.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tlt-infer` tool produces two outputs. \n",
    "The paths to the two outputs are exactly the same as the first `tlt-infer` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sample images from TensorRT inference.\n",
    "OUTPUT_PATH = 'data/faster_rcnn/inference_results_imgs_retrain' # relative path from $USER_EXPERIMENT_DIR.\n",
    "COLS = 3 # number of columns in the visualizer grid.\n",
    "IMAGES = 9 # number of images to visualize.\n",
    "\n",
    "visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
